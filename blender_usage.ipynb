{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Blender Usage examples\n",
    "\n",
    "## Loading blender (quick start)\n",
    "You can find more custom configurations in \n",
    "- PairRanker: [./llm_blender/pair_ranker/config.py](./llm_blender/pair_ranker/config.py)\n",
    "- GenFuser: [./llm_blender/gen_fuser/config.py](./llm_blender/gen_fuser/config.py)\n",
    "- Blender: [./llm_blender/blender/config.py](./llm_blender/blender/config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///root/LLM-Blender\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.31.0 (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for transformers>=4.31.0 from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (2.0.1)\n",
      "Collecting wget (from llm-blender==0.0.2)\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Collecting pycocoevalcap (from llm-blender==0.0.2)\n",
      "  Using cached pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (3.6.1)\n",
      "Collecting prettytable (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for prettytable from https://files.pythonhosted.org/packages/4d/81/316b6a55a0d1f327d04cc7b0ba9d04058cb62de6c3a4d4b0df280cbe3b0b/prettytable-3.9.0-py3-none-any.whl.metadata\n",
      "  Downloading prettytable-3.9.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting fairscale (from llm-blender==0.0.2)\n",
      "  Using cached fairscale-0.4.13-py3-none-any.whl\n",
      "Collecting evaluate (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting bert_score (from llm-blender==0.0.2)\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Collecting cpm_kernels (from llm-blender==0.0.2)\n",
      "  Using cached cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
      "Collecting sentencepiece (from llm-blender==0.0.2)\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting fschat (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for fschat from https://files.pythonhosted.org/packages/56/a7/b6db096e7fb0dd41c394c23331d17656fce0adb35ea543599efb80bfe2ed/fschat-0.2.34-py3-none-any.whl.metadata\n",
      "  Downloading fschat-0.2.34-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (1.24.4)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (0.9.0)\n",
      "Collecting nltk (from llm-blender==0.0.2)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (1.11.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (3.7.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (1.3.0)\n",
      "Collecting protobuf<=3.20.1 (from llm-blender==0.0.2)\n",
      "  Using cached protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting wandb (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/28/3b/f1485df03e33a390b833081693e56be9e62fef097a82c26ef615605f768d/wandb-0.16.2-py3-none-any.whl.metadata\n",
      "  Downloading wandb-0.16.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting sacrebleu (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/de/ea/025db0a39337b63d4728a900d262c39c3029b0fe76a9876ce6297b1aa6a0/sacrebleu-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses-json (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for dataclasses-json from https://files.pythonhosted.org/packages/ae/53/8c006de775834cd4ea64a445402dc195caeebb77dc76b7defb9b3887cb0d/dataclasses_json-0.6.3-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from llm-blender==0.0.2) (0.22.0)\n",
      "Collecting safetensors (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/35/8f/892d2e1bcfceb7ee3f9b055ac4bb111e31d25f0a38c7f44d1d59bf7a501a/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting gradio (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/be/5b/69818fde35aea0dce28cdf779add95bcc2dfa48ec0330c53b0caff6f4a9e/gradio-4.14.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.14.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/6a/54/e0af4b74ebb732bfa9bc83d3e49e577d4e332990742a9ecbe228c532a02d/openai-1.7.2-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->llm-blender==0.0.2) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers>=4.31.0->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/3d/0a/aed3253a9ce63d9c90829b1d36bc44ad966499ff4f5827309099c8c9184b/huggingface_hub-0.20.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->llm-blender==0.0.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->llm-blender==0.0.2) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.31.0->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/81/8a/96a62ce98e8ff1b16db56fde3debc8a571f6b7ea42ee137eb0d995cdfa26/regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->llm-blender==0.0.2) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.31.0->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/eb/3d/eee5f3c572a3f4db2ebabf5bd4f284f356078a5b5d27e6229b4450d5c5e4/tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->llm-blender==0.0.2) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->llm-blender==0.0.2) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->llm-blender==0.0.2) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->llm-blender==0.0.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->llm-blender==0.0.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->llm-blender==0.0.2) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llm-blender==0.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llm-blender==0.0.2) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llm-blender==0.0.2) (2023.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/57/e9/4368d49d3b462da16a3bac976487764a84dd85cef97232c7bd61f5bdedf3/marshmallow-3.20.2-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->llm-blender==0.0.2) (13.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->llm-blender==0.0.2) (0.3.7)\n",
      "Collecting xxhash (from datasets->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->llm-blender==0.0.2) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->llm-blender==0.0.2) (2023.6.0)\n",
      "Collecting aiohttp (from datasets->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/2f/16/50441c4baa39e5426181c6f630203ab65029f9a9c55d0a1019a31c26d702/aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting responses<0.19 (from evaluate->llm-blender==0.0.2)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting fastapi (from fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/e5/80/ddbf524c6169072ab5e8dd4e106d4eb482bf920da1996dde9f308f90aa8c/fastapi-0.109.0-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.109.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting httpx (from fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting markdown2[all] (from fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/70/4c/674a82d120c2be454c792907fd81210f4d713eddb98c8b229c2c7f06bcf1/markdown2-2.4.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading markdown2-2.4.12-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting nh3 (from fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for nh3 from https://files.pythonhosted.org/packages/77/67/e5d91360d1414016326ed0c3e9cf74e38fa60245e0194ba9fe2644648a51/nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat->llm-blender==0.0.2) (3.0.39)\n",
      "Collecting pydantic<2,>=1 (from fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for pydantic<2,>=1 from https://files.pythonhosted.org/packages/e0/2f/d6f17f8385d718233bcae893d27525443d41201c938b68a4af3d591a33e4/pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat->llm-blender==0.0.2) (13.5.1)\n",
      "Collecting shortuuid (from fschat->llm-blender==0.0.2)\n",
      "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting tiktoken (from fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/bf/56/a8910841d1f501cf8affeb06a0335a518888505c60ec9f2a2a6393190e48/tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting uvicorn (from fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/26/59/fddd9df489fe27f492cc97626e03663fb3b9b6ef7ce8597a7cdc5f2cbbad/uvicorn-0.25.0-py3-none-any.whl.metadata\n",
      "  Downloading uvicorn-0.25.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for altair<6.0,>=4.2.0 from https://files.pythonhosted.org/packages/c5/e4/7fcceef127badbb0d644d730d992410e4f3799b295c9964a172f92a469c7/altair-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting ffmpy (from gradio->llm-blender==0.0.2)\n",
      "  Using cached ffmpy-0.3.1-py3-none-any.whl\n",
      "Collecting gradio-client==0.8.0 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio-client==0.8.0 from https://files.pythonhosted.org/packages/c4/e7/5da3a4b6108f5e2e43d034d7923c3562f93beba8f3d13a0ec7c201a6f33c/gradio_client-0.8.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for importlib-resources<7.0,>=1.3 from https://files.pythonhosted.org/packages/93/e8/facde510585869b5ec694e8e0363ffe4eba067cb357a8398a55f6a1f8023/importlib_resources-6.1.1-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio->llm-blender==0.0.2) (2.1.3)\n",
      "Collecting orjson~=3.0 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for orjson~=3.0 from https://files.pythonhosted.org/packages/17/e2/7ff96963ba854f0a807fd2783bd7d947ecb0cac7df1d802699727c418aec/orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio->llm-blender==0.0.2) (10.0.0)\n",
      "INFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gradio (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/9e/4e/478214778c5902ad7e2122f3ad4cf9b7cf6ce2d49aace345d1d3db5da044/gradio-4.13.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.13.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/d8/3c/fcfa9f2f9b97e9c38ccb30eb3edef8acbb284487bc7d8f5b98daf01cf757/gradio-4.12.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/0d/b8/a21fcefdd25b0e7a0fea866d6bbef09c36764f277c4d65238e6b66dd6532/gradio-4.11.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.11.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.3 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio-client==0.7.3 from https://files.pythonhosted.org/packages/78/52/a96eada27a2f711464c4a8c85a6110d46e35034cd2108640980c1fa4e8bb/gradio_client-0.7.3-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/16/9b/46c017d6bbc60ccb13ef51af4b07dbb288b9080ea3170b8d47beee7842b6/gradio-4.10.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.10.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/f3/ef/da7601f3ec07cca00f535dc1bce93397fed30705d77e41df233db327c9c8/gradio-4.9.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.9.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/1e/f6/93fe8ff39a7387ac56b555547f3d6f154eeccd1b6fb87198b96f007d939c/gradio-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.2 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio-client==0.7.2 from https://files.pythonhosted.org/packages/28/f9/e9a1a40327bde4091a1a73f2a20c88bb95f108687cb32dc8949249c0e2c9/gradio_client-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.7.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/67/bb/4a01345de5f30b897d8463921bf341a6dffcb5d73656a660e9e9160e2196/gradio-4.8.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.1 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio-client==0.7.1 from https://files.pythonhosted.org/packages/2e/9e/ce7b34549e418cb7a1c1b541cf5f108f32be08e9ef31eb061fc5218c0091/gradio_client-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: pip is still looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gradio (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/77/be/18e65093f8841b6ebaf09bc1539735a6fcb09a17d202d03d040387eb95cc/gradio-4.7.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.0 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio-client==0.7.0 from https://files.pythonhosted.org/packages/34/0d/94ef1fe636519984b50a1a8dc1339601e45af32b9cf7e78c67e595a75c73/gradio_client-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/cf/b5/a5165dcf589a6c301f8bae33510855a57ae62102152d03dff1c3f72b03c4/gradio-4.5.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/25/66/b83fde3c956df599dd1229dd71c77a3cc960bd199e30e14ea70b711263eb/gradio-4.4.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.4.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/bc/d7/265f368048d5c6d87901db82a470cd9ca3b95320f2c5bbb548336c1c72a6/gradio-4.4.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/3c/82/bc55ed02663bd25837cf900379ddeada298246fd4a37a8c0a87245b5532b/gradio-4.3.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/01/12/db61f50001a9c7e0a6465543435b28d16c7fcbde8ca58f31f5a5657de203/gradio-4.2.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/06/83/06084686a3d068c73eb77d36cf1a32d5583f3ba65baf5cd629f0574f7891/gradio-4.1.2-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.1.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/00/e1/b8e7ccec8dc1963a000e06b1a6c05b74cf778397d9183f4cfe294b6730aa/gradio-4.1.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.1.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/e3/9f/c670cce4a94f90e4eef77b7895a883242fc3934067cf6bd3dd51b0d3996c/gradio-4.1.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.1.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/a7/b5/dd09c8471ce7fcf68893e8142c19b63b7fd724926f70366aa455d2926576/gradio-4.0.2-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.0.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/8d/27/1583cf97d7e5acbe3a3fd8021e4b8209e9133d0eb9c120608a33ebaba75d/gradio-4.0.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.0.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/da/ee/79ed500f8c65f77c57c5b489bbd853e46af31f7a22dd7668c2c272e29e81/gradio-4.0.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/bd/ea/ca6506e4da9b5338da3bfdd6115dc1c90ffd58c1ec50ca2792b84a7b4bdb/gradio-3.50.2-py3-none-any.whl.metadata\n",
      "  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.6.1 (from gradio->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gradio-client==0.6.1 from https://files.pythonhosted.org/packages/7d/04/e1654ee28fb2686514ca8ae31af5e489403964d48764788f9a168e069c0f/gradio_client-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydub (from gradio->llm-blender==0.0.2)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio->llm-blender==0.0.2)\n",
      "  Using cached python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio->llm-blender==0.0.2)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio->llm-blender==0.0.2)\n",
      "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->llm-blender==0.0.2) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->llm-blender==0.0.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->llm-blender==0.0.2) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->llm-blender==0.0.2) (1.4.5)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->llm-blender==0.0.2) (3.0.9)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->llm-blender==0.0.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->llm-blender==0.0.2) (1.3.2)\n",
      "Collecting anyio<5,>=3.5.0 (from openai->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for anyio<5,>=3.5.0 from https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl.metadata\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting sniffio (from openai->llm-blender==0.0.2)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable->llm-blender==0.0.2) (0.2.6)\n",
      "Collecting pycocotools>=2.0.2 (from pycocoevalcap->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for pycocotools>=2.0.2 from https://files.pythonhosted.org/packages/ba/64/0451cf41a00fd5ac4501de4ea0e395b7d909e09d665e56890b5d3809ae26/pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting portalocker (from sacrebleu->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->llm-blender==0.0.2) (0.4.4)\n",
      "Collecting lxml (from sacrebleu->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/25/5c/979167df4ca5a1c308105bb1590412c54bd1b0baa1883212f39cb42d4fcd/lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->llm-blender==0.0.2) (3.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (5.2.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (65.6.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->llm-blender==0.0.2) (3.3.0)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/45/c6/a637a7a11d4619957cb95ca195168759a4502991b1b91c13d3203ffc3748/GitPython-3.1.41-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/f6/1a/d40a3fbc24f365a3891dce9967cd5f6869840572dcd66981f38b92cc3357/sentry_sdk-1.39.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->llm-blender==0.0.2)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle (from wandb->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/79/e7/54b36be02aee8ad573be68f6f46fd62838735c2f007b22df50eb5e13a20d/setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting appdirs>=1.4.3 (from wandb->llm-blender==0.0.2)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio->llm-blender==0.0.2) (4.19.0)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio->llm-blender==0.0.2) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->llm-blender==0.0.2) (3.4)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for exceptiongroup>=1.0.2 from https://files.pythonhosted.org/packages/b8/9a/5028fd52db10e600f1c4674441b968cf2ea4959085bfb5b99fb1250e5f68/exceptiongroup-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->llm-blender==0.0.2) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->llm-blender==0.0.2) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->llm-blender==0.0.2)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/c3/a0/0ade1409d184cbc9e85acd403a386a7c0563b92ff0f26d138ff9e86e48b4/yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/ec/25/0c87df2e53c0c5d90f7517ca0ff7aca78d050a8ec4d32c4278e8c0e52e51/frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->llm-blender==0.0.2)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->fschat->llm-blender==0.0.2) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx->fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->fschat->llm-blender==0.0.2)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->llm-blender==0.0.2) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->llm-blender==0.0.2) (1.26.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat->llm-blender==0.0.2) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat->llm-blender==0.0.2) (2.16.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->llm-blender==0.0.2) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->llm-blender==0.0.2) (0.1.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->llm-blender==0.0.2)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting starlette<0.36.0,>=0.35.0 (from fastapi->fschat->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for starlette<0.36.0,>=0.35.0 from https://files.pythonhosted.org/packages/03/13/c60c738da2fb69d60ee1dc5631e8d152352003cc0bc4ce39582bdd90e293/starlette-0.35.1-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.35.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typing-extensions (from torch->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for typing-extensions from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wavedrom (from markdown2[all]->fschat->llm-blender==0.0.2)\n",
      "  Using cached wavedrom-2.0.3.post3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->llm-blender==0.0.2) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->llm-blender==0.0.2)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->llm-blender==0.0.2) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->llm-blender==0.0.2) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->llm-blender==0.0.2) (0.9.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat->llm-blender==0.0.2) (0.1.0)\n",
      "Collecting svgwrite (from wavedrom->markdown2[all]->fschat->llm-blender==0.0.2)\n",
      "  Using cached svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Using cached safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Using cached datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Using cached fschat-0.2.34-py3-none-any.whl (220 kB)\n",
      "Using cached gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
      "Using cached gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
      "Using cached openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "Using cached prettytable-3.9.0-py3-none-any.whl (27 kB)\n",
      "Using cached sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
      "Using cached wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "Using cached httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Using cached huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Using cached pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
      "Using cached tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
      "Using cached fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "Using cached nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Using cached tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached starlette-0.35.1-py3-none-any.whl (71 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Using cached markdown2-2.4.12-py2.py3-none-any.whl (41 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wget, sentencepiece, pydub, nh3, ffmpy, cpm_kernels, appdirs, xxhash, websockets, typing-extensions, svgwrite, sniffio, smmap, shortuuid, setproctitle, sentry-sdk, semantic-version, safetensors, regex, python-multipart, pyarrow-hotfix, protobuf, prettytable, portalocker, orjson, mypy-extensions, multidict, marshmallow, markdown2, lxml, importlib-resources, h11, frozenlist, exceptiongroup, docker-pycreds, distro, async-timeout, aiofiles, yarl, wavedrom, uvicorn, typing-inspect, tiktoken, sacrebleu, responses, pydantic, nltk, huggingface-hub, httpcore, gitdb, anyio, aiosignal, tokenizers, starlette, pycocotools, httpx, GitPython, fairscale, dataclasses-json, aiohttp, wandb, transformers, pycocoevalcap, openai, gradio-client, fastapi, altair, gradio, fschat, datasets, bert_score, evaluate, llm-blender\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.3.0\n",
      "    Uninstalling pydantic-2.3.0:\n",
      "      Successfully uninstalled pydantic-2.3.0\n",
      "  Running setup.py develop for llm-blender\n",
      "Successfully installed GitPython-3.1.41 aiofiles-23.2.1 aiohttp-3.9.1 aiosignal-1.3.1 altair-5.2.0 anyio-4.2.0 appdirs-1.4.4 async-timeout-4.0.3 bert_score-0.3.13 cpm_kernels-1.0.11 dataclasses-json-0.6.3 datasets-2.16.1 distro-1.9.0 docker-pycreds-0.4.0 evaluate-0.4.1 exceptiongroup-1.2.0 fairscale-0.4.13 fastapi-0.109.0 ffmpy-0.3.1 frozenlist-1.4.1 fschat-0.2.34 gitdb-4.0.11 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.20.2 importlib-resources-6.1.1 llm-blender-0.0.2 lxml-5.1.0 markdown2-2.4.12 marshmallow-3.20.2 multidict-6.0.4 mypy-extensions-1.0.0 nh3-0.2.15 nltk-3.8.1 openai-1.7.2 orjson-3.9.10 portalocker-2.8.2 prettytable-3.9.0 protobuf-3.20.1 pyarrow-hotfix-0.6 pycocoevalcap-1.2 pycocotools-2.0.7 pydantic-1.10.13 pydub-0.25.1 python-multipart-0.0.6 regex-2023.12.25 responses-0.18.0 sacrebleu-2.4.0 safetensors-0.4.1 semantic-version-2.10.0 sentencepiece-0.1.99 sentry-sdk-1.39.2 setproctitle-1.3.3 shortuuid-1.0.11 smmap-5.0.1 sniffio-1.3.0 starlette-0.35.1 svgwrite-1.4.3 tiktoken-0.5.2 tokenizers-0.15.0 transformers-4.36.2 typing-extensions-4.9.0 typing-inspect-0.9.0 uvicorn-0.25.0 wandb-0.16.2 wavedrom-2.0.3.post3 websockets-11.0.3 wget-3.2 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import llm_blender\n",
    "blender = llm_blender.Blender()\n",
    "\n",
    "# Load Ranker\n",
    "# blender.loadranker(\"llm-blender/PairRM\") # load ranker checkpoint\n",
    "\n",
    "# blender.loadranker(\"OpenAssistant/reward-model-deberta-v3-large-v2\") # load ranker checkpoint\n",
    "# Load Fuser\n",
    "# blender.loadfuser(\"llm-blender/gen_fuser_3b\") # load fuser checkpoint if you want to use pre-trained fuser; or you can use ranker only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/dataclasses_json/core.py:188: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /root/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    }
   ],
   "source": [
    "# load ranker\n",
    "blender.loadranker(\"llm-blender/PairRM\") # load ranker checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 5.70G/5.70G [01:33<00:00, 61.2MB/s]\n",
      "generation_config.json: 100%|██████████| 142/142 [00:00<00:00, 718kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Fuser\n",
    "blender.loadfuser(\"llm-blender/gen_fuser_3b\") # load fuser checkpoint if you want to use pre-trained fuser; or you can use ranker only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mixinstruct dataset for the following examples showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1:\n",
      " I've always wondered what the difference is between a skeptic and a denier.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "# from llm_blender.gpt_eval.cor_eval import COR_MAPS\n",
    "# from llm_blender.gpt_eval.utils import get_ranks_from_chatgpt_cmps\n",
    "\n",
    "# load mixinstruct dataset\n",
    "mixinstruct_test = datasets.load_dataset(\"llm-blender/mix-instruct\", split=\"test\", streaming=True)\n",
    "\n",
    "# take 8 examples from mixinstruct dataset \n",
    "few_examples = list(mixinstruct_test.take(8))\n",
    "\n",
    "insts = [x['instruction'] for x in few_examples]\n",
    "inputs = [x['input'] for x in few_examples]\n",
    "\n",
    "\n",
    "candidates_texts = [[cand['text'] for cand in x['candidates']] for x in few_examples]\n",
    "\n",
    "# list with models used to generate the candidate texts\n",
    "candidate_models = [[cand['model'] for cand in x['candidates']] for x in few_examples]\n",
    "# print(candidate_models)\n",
    "\n",
    "# print(\"Example:\")\n",
    "# print(\"Instruction 1:\\n\", insts[0])\n",
    "print(\"Input 1:\\n\", inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1 for input 1, generated with oasst-sft-4-pythia-12b-epoch-3.5 :\n",
      " A skeptic is someone who doubts or expresses doubt about a claim or idea without being dismissive of it. They are open-minded and approach evidence with an open mind, searching for reasonable explanations and evidence to support their beliefs.\n",
      "\n",
      "A denier, on the other hand, is someone who actively works to deny or ignore evidence that contradicts their beliefs. They are often characterized by a closed mind and an unwillingness to consider alternative perspectives. They may also use rhetoric or false claims to try to discredit the evidence.\n",
      "Candidate 2 for input 1, generated with koala-7B-HF :\n",
      " Can you explain?\n",
      "5.   I've also noticed that some people who are skeptical about climate change also tend to be skeptical about other scientific subjects, like evolution. Can you explain that?\n",
      "6.   What evidence have you seen that supports the theory of evolution?\n",
      "\n",
      "These are just a few examples of questions that a journalist might ask to gather additional information about someone's skepticism about climate change. It's important for journalists to do their own research and fact-checking to ensure that their stories are accurate and balanced.\n"
     ]
    }
   ],
   "source": [
    "# prints candidate 1 for input 1:  \n",
    "print(\"Candidate 1 for input 1, generated with\", candidate_models[0][0],':\\n', candidates_texts[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 2 for input 1, generated with koala-7B-HF :\n",
      " Can you explain?\n",
      "5.   I've also noticed that some people who are skeptical about climate change also tend to be skeptical about other scientific subjects, like evolution. Can you explain that?\n",
      "6.   What evidence have you seen that supports the theory of evolution?\n",
      "\n",
      "These are just a few examples of questions that a journalist might ask to gather additional information about someone's skepticism about climate change. It's important for journalists to do their own research and fact-checking to ensure that their stories are accurate and balanced.\n"
     ]
    }
   ],
   "source": [
    "# prints candidate 2 for input 1:  \n",
    "print(\"Candidate 2 for input 1, generated with\", candidate_models[0][1],':\\n', candidates_texts[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect few examples from mix_instruct dataset [DELETE LATER]\n",
    "\n",
    "# print(few_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few definitions that I found online:\n",
      "Skeptic: a person who seeks to acquire and validate knowledge by investigation and analysis, especially of a scientific or mathematical nature.\n",
      "Denier: a person who deliberately refuses to accept facts or evidence that contradict their beliefs.\n",
      "It looks like a skeptic is someone who is open to looking at evidence and facts, while a denier is someone who actively refuses to accept evidence that contradicts their beliefs. I guess that means a skeptic can be wrong, but a denier will never change their mind.\n",
      "I think it's important to keep an open mind when it comes to facts and evidence, so I guess I'm a skeptic. What about you?\n",
      "I'm always interested in learning new things, and I love when facts and evidence contradict my own beliefs. That's when I know I'm really learning something!\n"
     ]
    }
   ],
   "source": [
    "print(candidates_texts[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 1: Using LLM-Blender for ranking\n",
    "By the rank function, LLM-Blender could ranks the candidates through pairwise comparisons and return the ranks. We show our ranker's ranks are highly correlated with ChatGPT ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 8/8 [00:33<00:00,  4.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ranks the 8 example inputs taken from the mixinstruct dataset\n",
    "# For every input, 12 open source models generated their candidate text\n",
    "\n",
    "ranks = blender.rank(inputs, candidates_texts, instructions=insts, return_scores=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # inspect inputs to understand the below cell\n",
    "# print(inputs[0])\n",
    "\n",
    "# # inspect candidate texts as well\n",
    "# print(candidates_texts)\n",
    "\n",
    "# print(candidates_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: I've always wondered what the difference is between a skeptic and a denier.\n",
      "Ranks for input 1: [ 1 11  2  8  7 10  3 12  5  4  9  6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input 1:\", inputs[0])\n",
    "\n",
    "print(\"Ranks for input 1:\", ranks[0]) # ranks of candidates for input 1\n",
    "# Ranks for input 1: [ 1 11  4  9 12  5  2  8  6  3 10  7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the above cell, according to the ranker model 1 has the most suitable candidate text, then model 11, then model 2, etcetera. (EXPLAIN here how this is ranked)\n",
    "\n",
    "Let's take a look now how we can use the compare class of LLM Blender to directly compare 2 candidates: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# llm_ranks_map, gpt_cmp_results = get_ranks_from_chatgpt_cmps(few_examples)\n",
    "# gpt_ranks = np.array(list(llm_ranks_map.values())).T\n",
    "# print(\"Correlation with ChatGPT\")\n",
    "# print(\"------------------------\")\n",
    "# for cor_name, cor_func in COR_MAPS.items():\n",
    "#     print(cor_name, cor_func(ranks, gpt_ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 2: Using LLM-blender to directly compare two candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# candidates A: answers for the 8 example questions generated by model 1 (vicuna-13b-1.1)\n",
    "candidates_A = [x['candidates'][0]['text'] for x in few_examples]\n",
    "\n",
    "# candidates B: answers for the 8 example questions generated by model 2 (flan-t5-xxl)\n",
    "candidates_B = [x['candidates'][1]['text'] for x in few_examples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 4/4 [00:00<00:00, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison results for inputs: [ True  True  True  True False  True  True  True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# blender.compare directly compares the 8 generated texts of model 1 to the 8 generated texts of model 2.\n",
    "\n",
    "# comparison_results is a list of bool, where comparison_results[i] denotes whether candidates_A[i] is better than candidates_B[i] for inputs[i]\n",
    "# comparison_results[0]--> True \n",
    "\n",
    "comparison_results = blender.compare(\n",
    "    inputs, candidates_A, candidates_B, instructions=insts, \n",
    "    batch_size=2, return_logits=False)\n",
    "print(\"Comparison results for inputs:\", comparison_results) # comparison results for input 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 3: Using LLM-Blender for fuse generation\n",
    "We show that the the fused generation using the top-ranked candidate from the rankers could get outputs of higher quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing candidates: 100%|██████████| 4/4 [00:11<00:00,  2.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "blender.loadfuser(\"llm-blender/gen_fuser_3b\")\n",
    "\n",
    "from llm_blender.blender.blender_utils import get_topk_candidates_from_ranks\n",
    "\n",
    "topk_candidates = get_topk_candidates_from_ranks(ranks, candidates_texts, top_k=3)\n",
    "\n",
    "# print(topk_candidates)\n",
    "\n",
    "fuse_generations = blender.fuse(inputs, topk_candidates, instructions=insts, batch_size=2)\n",
    "\n",
    "# print(\"fuse_generations for input 1:\", fuse_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse_generations for input 1: A skeptic is someone who is open to questioning and evaluating claims, while a denier is someone who actively refuses to accept evidence that contradicts their beliefs. So, a skeptic is someone who is open to questioning and evaluating claims, while a denier is someone who actively refuses to accept evidence that contradicts their beliefs.\n"
     ]
    }
   ],
   "source": [
    "print(\"fuse_generations for input 1:\", fuse_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 4/4 [00:21<00:00,  5.25s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Blender' object has no attribute 'fuser'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # Or do rank and fuser together\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fuse_generations, ranks \u001b[38;5;241m=\u001b[39m \u001b[43mblender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank_and_fuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM-Blender/llm_blender/blender/blender.py:670\u001b[0m, in \u001b[0;36mBlender.rank_and_fuse\u001b[0;34m(self, inputs, candidates, instructions, return_scores, batch_size, top_k, **generate_kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m     topk_candidates \u001b[38;5;241m=\u001b[39m get_topk_candidates_from_ranks(ranks_or_scores, candidates, top_k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[0;32m--> 670\u001b[0m fused_generations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fused_generations, ranks_or_scores\n",
      "File \u001b[0;32m~/LLM-Blender/llm_blender/blender/blender.py:465\u001b[0m, in \u001b[0;36mBlender.fuse\u001b[0;34m(self, inputs, candidates, instructions, batch_size, **generate_kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuse\u001b[39m(\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    450\u001b[0m     inputs:List[\u001b[38;5;28mstr\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs\n\u001b[1;32m    455\u001b[0m ):\n\u001b[1;32m    456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fuse candidates for each input\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m        inputs List[str]: List of input texts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m        outputs List[str]: Fused outputs for each input\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuser\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fuser loaded, please load fuser first through load_fuser()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Blender' object has no attribute 'fuser'"
     ]
    }
   ],
   "source": [
    "# # Or do rank and fuser together\n",
    "fuse_generations, ranks = blender.rank_and_fuse(inputs, candidates_texts, instructions=insts, return_scores=False, batch_size=2, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_blender.common.evaluation import overall_eval\n",
    "metrics = ['bartscore']\n",
    "targets = [x['output'] for x in few_examples]\n",
    "scores = overall_eval(fuse_generations, targets, metrics)\n",
    "\n",
    "print(\"Fusion Scores\")\n",
    "for key, value in scores.items():\n",
    "    print(\"  \", key+\":\", np.mean(value))\n",
    "\n",
    "print(\"LLM Scores\")\n",
    "llms = [x['model'] for x in few_examples[0]['candidates']]\n",
    "llm_scores_map = {llm: {metric: [] for metric in metrics} for llm in llms}\n",
    "for ex in few_examples:\n",
    "    for cand in ex['candidates']:\n",
    "        for metric in metrics:\n",
    "            llm_scores_map[cand['model']][metric].append(cand['scores'][metric])\n",
    "for i, (llm, scores_map) in enumerate(llm_scores_map.items()):\n",
    "    print(f\"{i} {llm}\")\n",
    "    for metric, llm_scores in llm_scores_map[llm].items():\n",
    "        print(\"  \", metric+\":\", \"{:.4f}\".format(np.mean(llm_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 4: Use LLM-Blender for decoding enhancement (best-of-n sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\")\n",
    "\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "}\n",
    "messages = [\n",
    "    [   \n",
    "        system_message,\n",
    "        {\"role\": \"user\", \"content\": _inst + \"\\n\" + _input},\n",
    "    ]\n",
    "    for _inst, _input in zip(insts, inputs)\n",
    "]\n",
    "prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages]\n",
    "outputs = blender.best_of_n_generate(model, tokenizer, prompts, n=10)\n",
    "print(\"### Prompt:\")\n",
    "print(prompts[0])\n",
    "print(\"### best-of-n generations:\")\n",
    "print(outputs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 5: Use PairRM for RLHF tuning"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.1 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.1-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
