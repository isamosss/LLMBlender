{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Blender in Amazon Sagemaker\n",
    "\n",
    "This notebook will show you how to work with LLM Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). \n",
    "\n",
    "LLMs such as GPT-4 and Claude are closed-source, restricting insights into their architectures and training data. Open-source LLMs like LLaMA, Flan-T5 however, offer the option to be fine-tuned on custom instruction datasets, enabling the development of smaller yet efficient LLMs. As demonstrated in the LLM Blender paper, optimal LLMs for different examples can significantly vary and there is no open-source LLM that wint for every use case. Considering the diverse strengths and weaknesses of LLMs, it can be beneficial leverage an ensembling method that combines their complementary potentials, leading to improved robustness, generalization, accuracy, but also alleviating biases, errors, and uncertainties in individual LLMs, resulting in outputs better aligned with human preferences.\n",
    "\n",
    "LLM Blender is such an ensembling method, which is especially unique because, unlike the other RMs that encode and score each candidate respectively, PairRM takes a pair of candidates and compares them side-by-side to indentify the subtle differences between them.\n",
    "\n",
    "The LLM Blender framework consists of 2 components:\n",
    "1. A Ranker (PairRM): this model ranks the responses from multiple FMs through pairwise comparison (y1 + y2, y1 + y3, etc) and selects the top K candidates from different LLMs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. PairRM is based on microsoft/deberta-v3-large, and is therefore super efficient: 0.4B.\n",
    "\n",
    "2. A Fuser (GenFuser): this model fuses the top-ranked responses by combining them. GenFuser is essentially a fine-tuned Flan-T5-xl model, taking the input prompt (x) + the top K best ranked responses from PairRM and summarizes the top ranked responses.\n",
    "\n",
    "\n",
    "This notebook was tested on a ml.g5.12xlarge notebook instance with a Pytorch 2.0.1. GPU Optimized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install LLM blender\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n"
     ]
    }
   ],
   "source": [
    "import llm_blender\n",
    "blender = llm_blender.Blender()\n",
    "\n",
    "# load Ranker\n",
    "blender.loadranker(\"llm-blender/PairRM\") # load ranker checkpoint\n",
    "\n",
    "# Load Fuser\n",
    "blender.loadfuser(\"llm-blender/gen_fuser_3b\") # load fuser checkpoint if you want to use pre-trained fuser; or you can use ranker only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mixinstruct dataset\n",
    "\n",
    "To play around with LLM Blender, we will be using the Mixinstruct dataset. From this dataset, we will be using the input column, that contains a prompt or question we can ask an FM. It also contains a 'candidates' column, which for every entry contains a list of dictionaries in JSON format. This list contains answers from 12 different open source models to the input question. Because of this, we can use this dataset to test LLM Blender and rank + fuse the responses from different FMs to an input promp, and likely getting a more relevant response.  \n",
    "\n",
    "To learn more about MixInstruct, you can take a deeper look here: https://huggingface.co/datasets/llm-blender/mix-instruct?row=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1:\n",
      " I've always wondered what the difference is between a skeptic and a denier.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "\n",
    "# Load the test split of the MixInstruct dataset \n",
    "mixinstruct_test = datasets.load_dataset(\"llm-blender/mix-instruct\", split=\"test\", streaming=True)\n",
    "\n",
    "# Take 8 examples from MixInstruct dataset \n",
    "few_examples = list(mixinstruct_test.take(8))\n",
    "\n",
    "insts = [x['instruction'] for x in few_examples]\n",
    "inputs = [x['input'] for x in few_examples]\n",
    "\n",
    "candidates_texts = [[cand['text'] for cand in x['candidates']] for x in few_examples]\n",
    "\n",
    "# List with models used to generate the candidate texts\n",
    "\n",
    "candidate_models = [[cand['model'] for cand in x['candidates']] for x in few_examples]\n",
    "# print(candidate_models)\n",
    "\n",
    "# Print the first input question from the MixInstruct (test) dataset.\n",
    "print(\"Input 1:\\n\", inputs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "If you want, you can inspect the dataset and print all candidate responses (i.e. responses to the input from all 12 open source models in the dataset). However, this is a very long list, so we will just take a look at the candidate response of model 1 (oasst-sft-4-pythia-12b-epoch-3.5), model 2 (koala-7B-HF) and model 3 (alpaca native). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1 for input 1, generated with oasst-sft-4-pythia-12b-epoch-3.5 :\n",
      " A skeptic is someone who doubts or expresses doubt about a claim or idea without being dismissive of it. They are open-minded and approach evidence with an open mind, searching for reasonable explanations and evidence to support their beliefs.\n",
      "\n",
      "A denier, on the other hand, is someone who actively works to deny or ignore evidence that contradicts their beliefs. They are often characterized by a closed mind and an unwillingness to consider alternative perspectives. They may also use rhetoric or false claims to try to discredit the evidence.\n"
     ]
    }
   ],
   "source": [
    "# print all candidate responses to input 1 \n",
    "# print(\"All Candidate responses\",':\\n', candidates_texts[0])\n",
    "\n",
    "# prints candidate 1 for input 1:  \n",
    "print(\"Candidate 1 for input 1, generated with\", candidate_models[0][0],':\\n', candidates_texts[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 2 for input 1, generated with koala-7B-HF :\n",
      " Can you explain?\n",
      "5.   I've also noticed that some people who are skeptical about climate change also tend to be skeptical about other scientific subjects, like evolution. Can you explain that?\n",
      "6.   What evidence have you seen that supports the theory of evolution?\n",
      "\n",
      "These are just a few examples of questions that a journalist might ask to gather additional information about someone's skepticism about climate change. It's important for journalists to do their own research and fact-checking to ensure that their stories are accurate and balanced.\n"
     ]
    }
   ],
   "source": [
    "# prints candidate 2 for input 1:  \n",
    "print(\"Candidate 2 for input 1, generated with\", candidate_models[0][1],':\\n', candidates_texts[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 3 for input 1, generated with alpaca-native :\n",
      " Here are a few definitions that I found online:\n",
      "Skeptic: a person who seeks to acquire and validate knowledge by investigation and analysis, especially of a scientific or mathematical nature.\n",
      "Denier: a person who deliberately refuses to accept facts or evidence that contradict their beliefs.\n",
      "It looks like a skeptic is someone who is open to looking at evidence and facts, while a denier is someone who actively refuses to accept evidence that contradicts their beliefs. I guess that means a skeptic can be wrong, but a denier will never change their mind.\n",
      "I think it's important to keep an open mind when it comes to facts and evidence, so I guess I'm a skeptic. What about you?\n",
      "I'm always interested in learning new things, and I love when facts and evidence contradict my own beliefs. That's when I know I'm really learning something!\n"
     ]
    }
   ],
   "source": [
    "# prints candidate 3 for input 1:  \n",
    "print(\"Candidate 3 for input 1, generated with\", candidate_models[0][2],':\\n', candidates_texts[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank LLM outputs with the Ranker.\n",
    "\n",
    "Now we're ranking the candidate outputs from the 12 models in the MixInstruct dataset. We rank them for 8 example inputs with the rank function, PairRM, through pairwise comparisons. \n",
    "\n",
    "Then, we will inspect the ranking for input 1 ('I've always wondered what the difference is between a skeptic and a denier.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 8/8 [00:33<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ranks the 8 example inputs taken from the mixinstruct dataset\n",
    "# For every input, 12 open source models generated their candidate text\n",
    "\n",
    "ranks = blender.rank(inputs, candidates_texts, instructions=insts, return_scores=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: I've always wondered what the difference is between a skeptic and a denier.\n",
      "Ranks for input 1: [ 1 11  2  8  7 10  3 12  5  4  9  6]\n"
     ]
    }
   ],
   "source": [
    "# Input 1\n",
    "print(\"Input 1:\", inputs[0])\n",
    "\n",
    "# print the ranking for input 1.\n",
    "print(\"Ranks for input 1:\", ranks[0]) # ranks of candidates for input 1\n",
    "# Ranks for input 1: [ 1 11  4  9 12  5  2  8  6  3 10  7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the above cell, the ranker lists model 1 has the most suitable candidate text, then model 11, then model 2, etcetera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly compare two candidates\n",
    "\n",
    "Let's take a look now how we can use the compare class of LLM Blender to directly compare 2 candidates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Can you explain?\\n5.   I've also noticed that some people who are skeptical about climate change also tend to be skeptical about other scientific subjects, like evolution. Can you explain that?\\n6.   What evidence have you seen that supports the theory of evolution?\\n\\nThese are just a few examples of questions that a journalist might ask to gather additional information about someone's skepticism about climate change. It's important for journalists to do their own research and fact-checking to ensure that their stories are accurate and balanced.\", '5.   Write a short poem about the number and word.\\n6.   Ask students to turn and share their answers with the class.\\n2.   Review the answer key with the students and help them understand the concept.\\n3.   Have students work in small groups to play a matching game using the number and word cards.\\n4.   Review the results of the game and have students discuss the strategies they used to play.\\n5.   Have students create their own board games using the number and word cards, including the rules for the game and the scoring system.\\n6.   Allow students to share their games with the class and explain how they were created.\\n7.   Assign a writing prompt that asks students to reflect on their understanding of the concept and how they can apply it to their own lives.\\n\\nActivities and examples of how to explain the concept to students with Autism:\\n\\n1.   Provide clear and simple examples of the concepts being taught.\\n2.   Use visual aids such as graphic organizers and diagrams to explain the concepts.\\n3.   Provide opportunities for students to practice and apply the concepts in a low-pressure environment.\\n4.', 'to determine the remaining number of coins. Stay in the loop.', '2.   What is the role of government in monitoring air pollution?\\n3.   How do different countries approach monitoring air pollution?\\n4.   What are the challenges of monitoring air pollution in developing countries?\\n5.   What are the challenges of monitoring air pollution in densely populated cities?\\n6.   How has monitoring air pollution changed over time?\\n7.   What new technologies are being developed to monitor air pollution?\\n8.   What are the potential health impacts of air pollution?\\n9.   How can monitoring air pollution improve air quality and public health?\\n10.   What are the social and economic implications of monitoring air pollution?\\n11.   What are the ethical considerations of monitoring air pollution?\\n12.   What are the legal implications of monitoring air pollution?\\n13.   How do monitoring air pollution and public health inform environmental policy?\\n14.   What are the potential applications of monitoring air pollution?\\n15.   What are the future of air pollution monitoring?\\n16.   What are the limitations of air pollution monitoring?\\n17.   What', 'A: Yes, it is generally safe to eat a lot of peanut butter, as long as you do not have an allergy or intolerance to peanuts. Peanut butter contains healthy fats, protein, and beneficial nutrients, and it is often included in dietary recommendations for its nutritional value. However, if you have a peanut allergy or intolerance, it is important to avoid peanuts and peanut-containing products altogether.', '*   How to train a dog effectively\\n\\nII. Common dog behavior issues\\n\\n*   Understanding common dog behavior issues and how to resolve them\\n*   Common behavior problems in puppies\\n\\nIII. Dog health and wellness\\n\\n*   Overview of dog health and wellness\\n*   Common health concerns in dogs and how to prevent them\\n\\nIV. Dog grooming and care\\n\\n*   Overview of dog grooming and care\\n*   How to groom your dog at home\\n\\nV. Dog adoption\\n\\n*   Overview of dog adoption\\n*   How to find and adopt a new dog\\n\\nVI. Conclusion\\n\\n*   Recap of the main points covered in the guide\\n*   Final thoughts and tips for raising a well-socialized dog\\n\\nNote: This outline is just a starting point and can be adjusted as needed.', 'Spain - CityFrance - CountryParis - CityRome - CityGermany - Country', '2.   Directions to north using Google Maps on Android.\\n3.   How to create a route on Google Maps on Android.\\n4.   Google Maps on Android with offline maps.\\n5.   Google Maps on Android with offline navigation.\\n6.   Google Maps on Android with offline search.\\n7.   Google Maps on Android with offline business listings.\\n8.   How to use Google Maps on Android with offline location sharing.\\n9.   Google Maps on Android with offline walking directions.\\n10.   Google Maps on Android with offline transit directions.\\n\\n<ol start=\"11\"><li>Google Maps on Android as a GPS navigation device.</li><li>Google Maps on Android as a source of public transportation information.</li><li>Google Maps on Android as a tool for finding local businesses.</li><li>Google Maps on Android as a resource for finding new places to explore.</li><li>Google Maps on Android as a tool for finding the best route to a destination.</li><li>Google Maps on Android as a way to plan a trip.</li><li>Google Maps on Android as a tool for finding the best times']\n"
     ]
    }
   ],
   "source": [
    "# candidates A: answers for the 8 example questions generated by model 1 (vicuna-13b-1.1)\n",
    "candidates_A = [x['candidates'][0]['text'] for x in few_examples]\n",
    "\n",
    "# candidates B: answers for the 8 example questions generated by model 2 ()\n",
    "candidates_B = [x['candidates'][1]['text'] for x in few_examples]\n",
    "\n",
    "# print(candidates_A)\n",
    "#print(candidates_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 4/4 [00:00<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison results for inputs: [ True  True  True  True False  True  True  True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# blender.compare directly compares the 8 generated texts of model 1 to the 8 generated texts of model 2.\n",
    "# comparison_results is a list of bool, where comparison_results[i] denotes whether candidates_A[i] is better than candidates_B[i] for inputs[i]\n",
    "\n",
    "comparison_results = blender.compare(\n",
    "    inputs, candidates_A, candidates_B, instructions=insts, \n",
    "    batch_size=2, return_logits=False)\n",
    "print(\"Comparison results for inputs:\", comparison_results) # comparison results for input 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuse the top K candidates with GenFuser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing candidates: 100%|██████████| 4/4 [01:00<00:00, 15.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "# blender.loadfuser(\"llm-blender/gen_fuser_3b\")\n",
    "\n",
    "from llm_blender.blender.blender_utils import get_topk_candidates_from_ranks\n",
    "\n",
    "# First, we'll need to get the top K candidates from our 12 candidate responses in candidate_texts. We set K to 3 here. \n",
    "\n",
    "topk_candidates = get_topk_candidates_from_ranks(ranks, candidates_texts, top_k=3)\n",
    "# print(topk_candidates)\n",
    "\n",
    "# Fuse the top K candidates with the fuser class in LLM Blender. \n",
    "fuse_generations = blender.fuse(inputs, topk_candidates, instructions=insts, batch_size=2)\n",
    "\n",
    "# print(\"fuse_generations for input 1:\", fuse_generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse_generations for input 1: A skeptic is someone who is open to questioning and evaluating claims, while a denier is someone who actively refuses to accept evidence that contradicts their beliefs. So, a skeptic is someone who is open to questioning and evaluating claims, while a denier is someone who actively refuses to accept evidence that contradicts their beliefs.\n"
     ]
    }
   ],
   "source": [
    "print(\"fuse_generations for input 1:\", fuse_generations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The above output is a combination (fusion) of the top 3 best candidates in to our input question 'I've always wondered what's the difference between a skeptic and a denier'. Let's evaluate this output in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 4/4 [00:20<00:00,  5.24s/it]\n",
      "Fusing candidates: 100%|██████████| 4/4 [01:00<00:00, 15.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Or do rank and fuser together\n",
    "fuse_generations, ranks = blender.rank_and_fuse(inputs, candidates_texts, instructions=insts, return_scores=False, batch_size=2, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Then, we will evaluate whether the fused generation using the top-ranked candidates from the rankers gets outputs of higher quality. We evaluate this with the bartscore of the individual model outputs versus the bart score of the fused response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_score.pth trained on ParaBank not found.\n",
      "Please download bart_score.pth from bartscore github repo, then put it here:  /root/LLM-Blender/llm_blender/common/bart_score.pth\n",
      "Using the default bart-large-cnn model instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bartscore: 100%|██████████| 8/8 [00:00<00:00, 45.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion Scores\n",
      "   bartscore: -2.789272427558899\n",
      "LLM Scores\n",
      "0 oasst-sft-4-pythia-12b-epoch-3.5\n",
      "   bartscore: -3.8071\n",
      "1 koala-7B-HF\n",
      "   bartscore: -4.5505\n",
      "2 alpaca-native\n",
      "   bartscore: -4.2063\n",
      "3 llama-7b-hf-baize-lora-bf16\n",
      "   bartscore: -3.9364\n",
      "4 flan-t5-xxl\n",
      "   bartscore: -4.9341\n",
      "5 stablelm-tuned-alpha-7b\n",
      "   bartscore: -4.4329\n",
      "6 vicuna-13b-1.1\n",
      "   bartscore: -4.2022\n",
      "7 dolly-v2-12b\n",
      "   bartscore: -4.4400\n",
      "8 moss-moon-003-sft\n",
      "   bartscore: -3.5876\n",
      "9 chatglm-6b\n",
      "   bartscore: -3.7075\n",
      "10 mpt-7b\n",
      "   bartscore: -4.1353\n",
      "11 mpt-7b-instruct\n",
      "   bartscore: -4.2827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llm_blender.common.evaluation import overall_eval\n",
    "import numpy as np\n",
    "\n",
    "metrics = ['bartscore']\n",
    "targets = [x['output'] for x in few_examples]\n",
    "scores = overall_eval(fuse_generations, targets, metrics)\n",
    "\n",
    "print(\"Fusion Scores\")\n",
    "for key, value in scores.items():\n",
    "    print(\"  \", key+\":\", np.mean(value))\n",
    "\n",
    "print(\"LLM Scores\")\n",
    "llms = [x['model'] for x in few_examples[0]['candidates']]\n",
    "llm_scores_map = {llm: {metric: [] for metric in metrics} for llm in llms}\n",
    "for ex in few_examples:\n",
    "    for cand in ex['candidates']:\n",
    "        for metric in metrics:\n",
    "            llm_scores_map[cand['model']][metric].append(cand['scores'][metric])\n",
    "for i, (llm, scores_map) in enumerate(llm_scores_map.items()):\n",
    "    print(f\"{i} {llm}\")\n",
    "    for metric, llm_scores in llm_scores_map[llm].items():\n",
    "        print(\"  \", metric+\":\", \"{:.4f}\".format(np.mean(llm_scores)))\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.1 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.1-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
